{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ChatBot**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import pyttsx3\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import nbimporter\n",
    "import Audio_Cleaning\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "import import_ipynb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() # creating instance of lemmatizer - reducing a word to its base or dictionary form\n",
    "\n",
    "intents = json.loads(open('Intent English.json').read())\n",
    "\n",
    "# From training.ipynb\n",
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "model = load_model('chatbotmodel.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Function for Cleaning Up the Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)                               # splits a given sentence into words\n",
    "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]    # for each word reduce to its base form\n",
    "    return sentence_words                                                       # return list of words reduced to its base form"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Function for Bag-of-Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentence):                             # A representation of text that describes the occurrence of words within the sentence\n",
    "    sentence_words = cleaning_up_sentence(sentence)     # Go to the function above to get a list of words reduced to its base form\n",
    "    bag = [0] * len(words)                              # List of 0 length of words (from training.ipynb)\n",
    "\n",
    "    for w in sentence_words:                            # for word in list of words reduced to its base form\n",
    "        for i, word in enumerate(words):                # i = iteration, w = list element (word)\n",
    "            if word == w:                               # if the words in both lists match \n",
    "                bag[i] = 1                              # change particular index in bag from 0 to 1 \n",
    "    \n",
    "    return np.array(bag)                                # return array of bag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Function for Predicting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence)                                                # Go to the function above to get a representation of text that describes the occurrence of words within the sentence\n",
    "    res = model.predict(np.array([bow]))[0]                                     # Based of the array of bag-of-words get the model to predict the response\n",
    "\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    result = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]         # If r is greater than the error threshold, get the index and the response probability and make into a list item\n",
    "\n",
    "    result.sort(key = lambda x: x[1], reverse = True)                           # Sort the list from highest probability first\n",
    "    return_list = []                                                            # Empty list\n",
    "\n",
    "    for r in result:                                                            # For each potential result in result list\n",
    "        return_list.append({'intent': classes[r[0]], 'probability': str(r[1])}) # Get the tag (from .json file) and the probability of the response and append as list item to the empty list created above\n",
    "    \n",
    "    return return_list                                                          # Return the list with tag and probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Getting Response Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(intents_list, intents_json):       # Get the list with tag and probability & the .json file \n",
    "    tag = intents_list[0]['intent']                 # Get the tag of the first element of the list with tag and prob.\n",
    "    list_of_intents = intents_json['intents']       # Get the contents of the .jsn that fall under intents (basically all the info comprised within) \n",
    "\n",
    "    for i in list_of_intents:                       # Go section by section\n",
    "        if i['tag'] == tag:                         # If tag matches the one found above in the first element of the list with tag and prob\n",
    "            result = random.choice(i['responses'])  # Randomly choose a response from that tag section\n",
    "            break                                   # Break and exist the loop\n",
    "    \n",
    "    return result                                   # Return the randomly choosen response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO! Bot is running!\n",
      "Speak Now...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'text_en' (str)\n",
      "Stored 'text_fr' (str)\n",
      "Stored 'text_es' (str)\n",
      "You: bonjour comment ça va\n",
      "Translated FR 2 EN:  Hello how are you\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "Bot: Bien sûr, veuillez remettre vos documents pour inspection.\n",
      "\n",
      "Speak Now...\n",
      "Stored 'text_en' (str)\n",
      "Stored 'text_fr' (str)\n",
      "Stored 'text_es' (str)\n",
      "You: hola cómo estás\n",
      "Translated ES 2 EN:  Hi, how are you\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Bot: ¡Excelente! Verificaré sus documentos de inmediato.\n",
      "\n",
      "Speak Now...\n",
      "Stored 'text_en' (str)\n",
      "Stored 'text_fr' (str)\n",
      "Stored 'text_es' (str)\n",
      "You: I think I lost my passport\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Bot: It happens to the best of us. We have a lost and found desk after security. Please report your lost item there.\n",
      "\n",
      "Speak Now...\n",
      "Stored 'text_en' (str)\n",
      "Stored 'text_fr' (str)\n",
      "Stored 'text_es' (str)\n",
      "You: where is terminal B\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Bot: Sure thing! To get to Terminal B, simply follow the signs. The security checkpoint is ahead, and you'll find the departure gates clearly indicated.\n",
      "\n",
      "Speak Now...\n",
      "Stored 'text_en' (str)\n",
      "Stored 'text_fr' (str)\n",
      "Stored 'text_es' (str)\n",
      "You: exit please\n"
     ]
    }
   ],
   "source": [
    "languages = ['en', 'fr', 'es']  # List of potential languages\n",
    "print('GO! Bot is running!')        \n",
    "\n",
    "while True:\n",
    "    # %run ./Testing.ipynb\n",
    "    # %run ./Speech_Lang_Detector.ipynb\n",
    "    print('Speak Now...') \n",
    "    %run ./Audio_Cleaning.ipynb\n",
    "\n",
    "    possible_langs = Audio_Cleaning.lang_detect(text_en, text_fr, text_es)\n",
    "    lang_list, prob_list = Audio_Cleaning.lang_prob(possible_langs)\n",
    "    lang_ISO = Audio_Cleaning.ISO_639(lang_list, prob_list)\n",
    "\n",
    "    # English\n",
    "    if languages.index(lang_ISO) == 0:\n",
    "        message = text_en\n",
    "        print('You: {}'.format(message))\n",
    "\n",
    "        if message.lower() == 'exit please':\n",
    "            break\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        ints = predict_class(message)\n",
    "        res = get_response(ints, intents)\n",
    "        print('Bot: {}\\n'.format(res))\n",
    "\n",
    "        engine = pyttsx3.init()\n",
    "        voices = engine.getProperty('voices')\n",
    "        engine.setProperty('voice', voices[1].id)\n",
    "        engine.say(res)\n",
    "        engine.runAndWait()\n",
    "\n",
    "    # French\n",
    "    elif languages.index(lang_ISO) == 1:\n",
    "        message = text_fr\n",
    "        print('You: {}'.format(message))\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        translator_fr2en = GoogleTranslator(source = 'auto', target = 'en')\n",
    "        fr2en = translator_fr2en.translate(message)\n",
    "        print('Translated FR 2 EN: ', translator_fr2en.translate(message))\n",
    "\n",
    "        if fr2en.lower() == 'exit please':\n",
    "            break\n",
    "\n",
    "        ints = predict_class(fr2en)\n",
    "        res = get_response(ints, intents)\n",
    "        translator_en2fr = GoogleTranslator(source = 'auto', target = 'fr')\n",
    "        res_en2fr = translator_en2fr.translate(res)\n",
    "        print('Bot: {}\\n'.format(res_en2fr))\n",
    "\n",
    "        engine = pyttsx3.init()\n",
    "        voices = engine.getProperty('voices')\n",
    "        engine.setProperty('voice', voices[3].id)\n",
    "        engine.say(res_en2fr)\n",
    "        engine.runAndWait()\n",
    "\n",
    "    # Spanish\n",
    "    # if languages.index(lang_ISO) == 2:\n",
    "    else:\n",
    "        message = text_es\n",
    "        print('You: {}'.format(message))\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        translator_es2en = GoogleTranslator(source = 'auto', target = 'en')\n",
    "        es2en = translator_es2en.translate(message)\n",
    "        print('Translated ES 2 EN: ', translator_es2en.translate(message))\n",
    "\n",
    "        if es2en.lower() == 'exit please':\n",
    "            break\n",
    "\n",
    "        ints = predict_class(es2en)\n",
    "        res = get_response(ints, intents)\n",
    "        translator_en2es = GoogleTranslator(source = 'auto', target = 'es')\n",
    "        res_en2es = translator_en2es.translate(res)\n",
    "        print('Bot: {}\\n'.format(res_en2es))\n",
    "\n",
    "        engine = pyttsx3.init()\n",
    "        voices = engine.getProperty('voices')\n",
    "        engine.setProperty('voice', voices[2].id)\n",
    "        engine.say(res_en2es)\n",
    "        engine.runAndWait()\n",
    "\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
